{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 05. 데이터전처리(Data Preprocessing)\n",
        "결손값. 즉 NaN,Null 값은 허용되지 않음\n",
        " - 결손값이 적은 경우: null값을 피처의 평균값으로 대체\n",
        " - 결손값이 많은 경우: 피처를 드롭\n",
        " - 결손값이 일정 수준 이상: 해당 피처가 중요도가 높은 피처이고 Null을 단순히 피처의 평균값으로 대체할 경우 예측 왜곡이 심할 수 있다면 업무로직 등을 상세히 검토해 더 정밀한 대체 값을 선정\n",
        "\n",
        " 문자열 값을 입력으로 허용x\n",
        "  - 문자열 값은 인코딩돼서 숫자형으로 변환\n",
        "  - 카테고리형 피러는 코드값으로 표현\n",
        "  - 텍스트형 피처는 피처 벡터화(feature vectorization) 등의 기법으로 벡터화하거나 불필요한 피처라고 판단되면 삭제\n"
      ],
      "metadata": {
        "id": "aCPh17dG9gVD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1) 데이터 인코딩\n",
        "1. 레이블 인코딩(Label encoding) :카테고리 피처를  코드형 숫자값으로 변환하는 것\n",
        " - LabelEincoder를 객체로 생성한 후 fit( )과 transform()을 호출해 수행\n",
        " - LabelEncoder 객체의 classes_ 속성값으로 어떤 숫자 값으로 인코딩됐는지 확인\n",
        " - inverse_transform(): 인코딩된 값을 다시 디코딩\n",
        " - 레이블인코딩이 일괄적인 숫자값으로 변환되면서 몇 ML 알 고리즘에는 예측 성능이 떨어지는 경우가 발생할 수 있음. ex.선형회귀 등에서 에러, 트리계열은 무관.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HNrADz-JuDyS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4id9EgmMH7IT"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "items=['TV','냉장고','전자레인지','컴퓨터','선풍기','선풍기','믹서','믹서']\n",
        "\n",
        "#LabelEncoder를 객체로 생성한 후, fit( )과 transform( )으로 레이블인코딩 수행.\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(items)\n",
        "labels = encoder.transform(items)\n",
        "print('인코딩 변환값:',labels)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('인코딩 클래스:', encoder.classes_)"
      ],
      "metadata": {
        "id": "W-yy8iKmwnDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('디코딩 원본값: ', encoder.inverse_transform([4,5,2,0,1,1,3,3]))"
      ],
      "metadata": {
        "id": "ACt3QsPlxLow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. 원-핫인코딩(One Hot encoding): 피처 값의 유형에 따라 새로운 피처를 추가해 고유값에 해당하는 칼럼에만 1을 표시하고 나머지 칼럼에는 0을 표시하는 방식\n",
        "- 사이킷런에서 OneHotEncoder 클래스로 쉽게 변환 가능.\n",
        "- 변환 전에 모든 문자열이 숫자형 값으로 변환돼야 함.\n",
        "- 입력 값으로 2차원 데이터가 필요.\n",
        "- get_dummies( ): 판다스에서 원=핫 인코딩을 더 쉽게 지원하는 API / 숫자형 변환 없이 바로 변환 가능."
      ],
      "metadata": {
        "id": "VEV6CaaRwoxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "\n",
        "items=['TV','냉장고','전자레인지','컴퓨터','선풍기','선풍기','믹서','믹서']\n",
        "\n",
        "#먼저 숫자 값으로 변환을 위해 LabelEncoder로 변환합니다.\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(items)\n",
        "labels =encoder.transform(items)\n",
        "# 2차원 데이터로 변환합니다.\n",
        "labels = labels.reshape(-1, 1)\n",
        "\n",
        "# 원- 핫 인코딩을 적용합니다.\n",
        "oh_encoder = OneHotEncoder()\n",
        "oh_encoder.fit(labels)\n",
        "oh_labels = oh_encoder.transform(labels)\n",
        "print( '원-핫 인코딩 데이터' )\n",
        "print(oh_labels.toarray())\n",
        "print('원-핫 인코딩 데이터 차원')\n",
        "print(oh_labels.shape)"
      ],
      "metadata": {
        "id": "lphi6ZVSy-Aq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({'item':['TV','냉장고','전자레인지','컴퓨터','선풍기','선풍기','믹서','믹서']\n",
        "                  })\n",
        "pd.get_dummies(df)"
      ],
      "metadata": {
        "id": "dA3QMw2dzuZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2) 피처 스케일링과 정규화\n",
        "1. 피처 스케일링(feature scaling): 서로 다른 변수의 값 범위를 일정한 수준으로 맞추는 작업\n",
        "  1. 표준화(Standardization) :데이터의 피처 각각이 평균이 0이고 분산이 1인 가우시안 정규 분포를 가진 값으로 변환\n",
        "  - Xi_new = Xi - mean(x) / stdev(x)\n",
        "\n",
        "  2. 정규화(Normalization): 서로 다른 피처의 크기를 통일하기 위해 크기를 변환해주는 개념\n",
        "  - Xi_new = Xi - min(x) / max(x) - min(x)\n",
        "\n",
        "  사이킷런에서의 Normalizer 모듈은 선형대수에서의 정규화 개념이 적용됐으며, 개별 벡터의 크기를 맞추기 위해 변환하는 것을 의미\n",
        "\n",
        "  - Xi_new = Xi / sqrt(xi**2 + yi**2 +zi**2)\n",
        "\n",
        "2. StandardScaler: 개별 피처를 평균이 0이고, 분산이 1인 값으로 변환해주는 클래스(매우 중요한 단계)\n"
      ],
      "metadata": {
        "id": "tFIRDFoU0L6W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "\n",
        "# 붓꽃 데이터 세트를 로딩하고 DataFrame으로 변환합니다.\n",
        "iris = load_iris()\n",
        "iris_data = iris.data\n",
        "iris_df = pd.DataFrame(data=iris_data, columns=iris.feature_names)\n",
        "\n",
        "print('feature들의 평균값')\n",
        "print(iris_df.mean())\n",
        "print('\\nfeature들의 분산값')\n",
        "print(iris_df.var())"
      ],
      "metadata": {
        "id": "AaQYsvmM0LqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# StandardScaler객체 생성\n",
        "scaler = StandardScaler()\n",
        "#StandardScaler로데이터세트변환. fit()과 transform()호출.\n",
        "scaler.fit(iris_df)\n",
        "iris_scaled = scaler. transform(iris_df)\n",
        "\n",
        "#transform()시 스케일 변환된 데이터 세트가 Numpy ndarray로 반환돼 이를 DataFrame으로 변환\n",
        "iris_df_scaled = pd.DataFrame(data=iris_scaled, columns=iris.feature_names)\n",
        "print('feature들의 평균값')\n",
        "print(iris_df_scaled.mean())\n",
        "print('\\nfeature들의 분산값')\n",
        "print(iris_df_scaled.var())"
      ],
      "metadata": {
        "id": "MRh7TCFD2oEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. MinMaxScaler: 데이터 값을 0과 1 사이의 범위 값 으로 변환(음수 값이 있으면 -1에서 1값으로 변환), 데이터의 분포가 가우시안 분포가 아닐 경우 적용.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_CuY0mO60ZGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn .preprocessing import MinMaxScaler\n",
        "\n",
        "# MinMaxScaler 객체 생성\n",
        "scaler = MinMaxScaler()\n",
        "# MinMaxScaler로 데이터 세트 변환. fit()과 transform() 호출.\n",
        "scaler.fit(iris_df)\n",
        "irisscaled =scaler.transform(iris_df)\n",
        "\n",
        "# transform()시 스케일 변환된 데이터 세트가 Numpy ndarray로 반환돼 이를DataFrame으로 변환\n",
        "iris_df_scaled = pd.DataFrame(data=iris_scaled, columns=iris.feature_names)\n",
        "print('feature들의 최솟값')\n",
        "print(iris_df_scaled.min())\n",
        "print('\\nfeature들의 최댓값')\n",
        "print(iris_df_scaled.max())"
      ],
      "metadata": {
        "id": "6fm3pQaK3Rba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. 학습 데이터와 테스트 데이터의 스케일링 변환 시 유의점\n",
        "  1. 가능하다면 전체 데이터의 스케일링 변환을 적용한 뒤 학습과 테스트 데이터로 분리\n",
        "  2.1이 여의치 않다면 테스트 데이터 변환 시에는 fit()이나 fit_transform()을 적용하지 않고 학습 데이터로 이미 fit()된 Scaler객체를 이용해 transiorm( )으로 변환"
      ],
      "metadata": {
        "id": "Bh_Ju0CL3rt0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "#학습 데이터는 0부터 10까지, 테스트 데이터는 0부터 5까지 값을 가지는 데이터 세트로 생성\n",
        "#Scaler 클래스의 fit(), transform()은 2차원 이상 데이터만 가능하므로 reshape(-1, 1)로 차원 변경\n",
        "train_array =np.arange(0, 11).reshape(-1, 1)\n",
        "test_array = np.arange (0, 6).reshape(-1, 1)"
      ],
      "metadata": {
        "id": "t9ZhKFRC3rc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#MinMaxScaler 객체에 별도의 feature_range 파라미터 값을 지정하지 않으면 0~1값으로 변환\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "#fit()하게 되면 train_array 데이터의 최솟값이 0, 최댓값이 10으로 설정.\n",
        "scaler.fit(train_array)\n",
        "\n",
        "#1/10 scale로 train_array 데이터 변환함. 원본10 -〉 1로 변환됨.\n",
        "train_scaled = scaler. transform(train_array)\n",
        "\n",
        "print('원본 train_array 데이터:', np.round(train_array.reshape(-1), 2))\n",
        "print('Scale된 train_array 데이터:', np.round(train_scaled.reshape(-1), 2))"
      ],
      "metadata": {
        "id": "_mRSXO5e4nLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#MinMaxScaler에 test_array를 fit()하게 되면 원본 데이터의 최솟값이 0, 최댓값이 5로 설정됨\n",
        "scaler.fit(test_array)\n",
        "\n",
        "#1/5scale로 test_array 데이터 변환함. 원본 5->1로 변환.\n",
        "test_scaled = scaler. transform(test_array)\n",
        "\n",
        "#test_array의 scale 변환 출력.\n",
        "print('원본 test_array 데이터:',np.round(test_array.reshape(-1), 2))\n",
        "print('Scale된 test_array 데이터:', np. round(test_scaled.reshape(-1), 2))"
      ],
      "metadata": {
        "id": "SG5cym525DbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습 데이터와 테스트 데이터의 스케일링이 맞지 않음.\n",
        "이렇게 되면 학습 데이터와 테스트 데이터의 서로 다른 원본 값이 동일한 값으로 변환되는 결과를 초래.\n",
        "따라서 머신러닝 모델은 학습 데이터를 기반으로 학습되기 때문에 반드시 테스트 데이터는 학습 데이터의 스케일링 기준에 따라야 함."
      ],
      "metadata": {
        "id": "bi_6Ece25ggZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = MinMaxScaler()\n",
        "scaler.fit(train_array)\n",
        "train_scaled = scaler. transform(train_array)\n",
        "print('원본 train_array 데이터:', np.round(train_array.reshape(-1), 2))\n",
        "print('Scale된 train_array 데이터: ' , np.round(train_scaled.reshape(-1), 2))\n",
        "\n",
        "# test_array에 Scale 변환을 할 때는 반드시 fit()을 호출하지 않고 transform()만으로 변환해야함.\n",
        "test_scaled = scaler.transform(test_array)\n",
        "print('\\n원본 test_array 데이터: ',np.round(test_array.reshape(-1),2))\n",
        "print('Scale된 test_array 데이터:', np.round(test_scaled.reshape(-1), 2))"
      ],
      "metadata": {
        "id": "JOhhb2jo557q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. 사이킷런으로 수행하는 타이타닉 생존자 예측"
      ],
      "metadata": {
        "id": "3KsPOe8MHf_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "titanic_df = pd.read_csv('/content/titanic.csv')\n",
        "titanic_df.head(3)"
      ],
      "metadata": {
        "id": "atBevycJHftA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\n 학습데이터정보###\\n')\n",
        "print(titanic_df.info())"
      ],
      "metadata": {
        "id": "nAQM-U5zIAcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "titanic_df['Age'].fillna(titanic_df['Age'].mean(), inplace=True)\n",
        "titanic_df['Cabin'].fillna('N', inplace=True)\n",
        "titanic_df['Embarked'].fillna('N', inplace=True)\n",
        "print('데이터 세트 Null 값 개수', titanic_df.isnull().sum().sum())"
      ],
      "metadata": {
        "id": "fKKxQn7NIGwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(' Sex 값 분포 :\\n', titanic_df['Sex'].value_counts())\n",
        "print('\\n Cabin 값 분포 :\\n', titanic_df['Cabin'].value_counts())\n",
        "print('\\n Embarked 값 분포: \\n', titanic_df['Embarked'].value_counts())"
      ],
      "metadata": {
        "id": "AkUGrrWMIV0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "titanic_df['Cabin'] = titanic_df['Cabin'].str[:1]\n",
        "print(titanic_df['Cabin'].head(3))"
      ],
      "metadata": {
        "id": "SZV2u-isIrLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "titanic_df.groupby(['Sex', 'Survived'])['Survived'].count()"
      ],
      "metadata": {
        "id": "yrkJXvhTIw2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.barplot(x= 'Sex', y= 'Survived', data = titanic_df)"
      ],
      "metadata": {
        "id": "ZuIH501dIzlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.barplot(x= 'Pclass', y= 'Survived', hue = 'Sex', data = titanic_df)"
      ],
      "metadata": {
        "id": "2Xhtgmr0d8FU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 입력 age에 따라 구분 값을 반환하는 함수 설정. Dataframe의 apply lambda식에 사용.\n",
        "def get_category (age):\n",
        "  cat = ' '\n",
        "  if age <= -1: cat = 'Unknown'\n",
        "  elif age <= 5: cat = 'Baby'\n",
        "  elif age <= 12: cat = 'Child'\n",
        "  elif age <= 18: cat = 'Teenager'\n",
        "  elif age <= 25: cat = 'Student'\n",
        "  elif age <= 35 : cat = 'YoungAdult'\n",
        "  elif age <= 60: cat = 'Adult'\n",
        "  else : cat = 'Elderly'\n",
        "\n",
        "  return cat"
      ],
      "metadata": {
        "id": "LdRiahyqIzQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#막대 그래프의 크기 figure를 더 크게 설정\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# X축의 값을 순차적으로 표시하기 위한 설정\n",
        "group_names =['Unknown','Baby,' 'Child', 'Teenager', 'Student', 'Young Adult','Adult','Elderly']\n",
        "\n",
        "#lambda식에 위에서 생성한 get_category( )함수를 반환 값으로 지정.\n",
        "#get_category(x)는 입력 값으로 'Age' 칼럼 값을 받아서 해당하는 Cat 반환\n",
        "titanic_df['Age_cat'] = titanic_df['Age'].apply(lambda x : get_category(x))\n",
        "sns.barplot(x='Age_cat', y='Survived', hue='Sex', data=titanic_df, order=group_names)\n",
        "titanic_df .drop('Age_cat', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "_VitrGOlJXyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "def encode_features(dataDF):\n",
        "  features = ['Cabin', 'Sex', 'Embarked']\n",
        "  for feature in features:\n",
        "    le = preprocessing.LabelEncoder()\n",
        "    le = le.fit(dataDF[feature])\n",
        "    dataDF[feature] = le.transform(dataDF[feature])\n",
        "  return dataDF\n",
        "\n",
        "titanic_df =encode_features(titanic_df)\n",
        "titanic_df.head()"
      ],
      "metadata": {
        "id": "7qxnx6ixJ2ah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Null 처리 함수\n",
        "def fillna(df):\n",
        "  df['Age'].fillna(df['Age'].mean(), inplace=True)\n",
        "  df['Cabin'].fillna('N', inplace=True)\n",
        "  df['Embarked'].fillna('N', inplace=True)\n",
        "  df['Fare'].fillna(0, inplace=True)\n",
        "  return df\n",
        "\n",
        "# 머신러닝 알고리즘에 불필요한 속성 제거\n",
        "def drop_features(df):\n",
        "  df.drop(['PassengerId', 'Name', 'Ticket'], axis=1, inplace=True)\n",
        "  return df\n",
        "\n",
        "# 레이블 인코딩 수행\n",
        "def format_features(df):\n",
        "  df['Cabin'] =df['Cabin'].str[:1]\n",
        "  features = ['Cabin', 'Sex', 'Embarked']\n",
        "  for feature in features:\n",
        "    le = LabelEncoder()\n",
        "    le = le.fit(df[feature])\n",
        "    df[feature] = le.transform(df[feature])\n",
        "  return df\n",
        "\n",
        "# 앞에서 설정한 데이터 전처리 함수 호출\n",
        "def transform_features(df):\n",
        "  df = fillna(df)\n",
        "  df = drop_features(df)\n",
        "  df = format_features(df)\n",
        "  return df"
      ],
      "metadata": {
        "id": "Fi0pLIRYKD0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 원본 데이터를 재로딩하고, 피처 데이터 세트와 레이블 데이터 세트 추출.\n",
        "titanic_df =pd.read_csv('/content/titanic.csv')\n",
        "y_titanic_df = titanic_df['Survived']\n",
        "X_titanic_df= titanic_df.drop('Survived', axis=1)\n",
        "\n",
        "X_titanic_df = transform_features(X_titanic_df)"
      ],
      "metadata": {
        "id": "femIIJFGKptO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test=train_test_split(X_titanic_df, y_titanic_df,test_size=0.2, random_state=11)"
      ],
      "metadata": {
        "id": "R7MYeSlQKzOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn. tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 결정트리, RandomForest, 로지스틱 회귀를 위한 사이킷런 Classifier 클래스 생성\n",
        "dt_clf = DecisionTreeClassifier(random_state=11)\n",
        "rf_clf =RandomForestClassifier(random_state=11)\n",
        "lr_clf = LogisticRegression()\n",
        "\n",
        "# DecisionTreeClassifier 학습/예측/평가\n",
        "dt_clf.fit(X_train, y_train)\n",
        "dt_pred =dt_clf.predict(X_test)\n",
        "print( 'DecisionTreeClassifier 정확도 : {0:4f}'.format(accuracy_score(y_test, dt_pred)))\n",
        "\n",
        "# RandomForestClassifier 학습/예측/평가\n",
        "rf_clf.fit(X_train, y_train)\n",
        "rf_pred =rf_clf.predict(X_test)\n",
        "print('RandomForestClassifier 정확도 :{0:4f}'.format(accuracy_score(y_test, rf_pred)))\n",
        "\n",
        "# LogisticRegression 학습/예측/평가\n",
        "lr_clf.fit(X_train, y_train)\n",
        "lr_pred = lr_clf.predict(X_test)\n",
        "print('LogisticRegression 정확도 : {0:.4f}'.format(accuracy_score(y_test, lr_pred)))\n"
      ],
      "metadata": {
        "id": "qSn6As3eK4HS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn. model_selection import KFold\n",
        "\n",
        "def exec_kfold(clf, folds=5):\n",
        "  #폴드세트를 5개인 KFold 객체를 생성, 폴드 수만큼 예측 결과 저장을 위한 리스트 객체 생성.\n",
        "  kfold = KFold(n_splits=folds)\n",
        "  scores = []\n",
        "\n",
        "  # KFold 교차 검증 수행.\n",
        "  for iter_count, (train_index, test_index) in enumerate(kfold.split(X_titanic_df)):\n",
        "    # X_titanic_df 데이터에서 교차검증 별로 학습과 검증 데이터를 가리키는 index 생성\n",
        "    X_train, X_test = X_titanic_df.values[train_index], X_titanic_df.values[test_index]\n",
        "    y_train, y_test = y_titanic_df.values[train_index], y_titanic_df.values[test_index]\n",
        "    # Classifier 학습, 예측, 정확도 계산\n",
        "    clf.fit(X_train, y_train)\n",
        "    predictions = clf.predict(X_test)\n",
        "    accuracy =accuracy_score(y_test, predictions)\n",
        "    scores.append(accuracy)\n",
        "    print(\"교차검증{0}정확도: {1:.4f}\".format(iter_count,accuracy))\n",
        "\n",
        "  # 5개 fold에서의 평균 정확도 계산.\n",
        "  mean_score = np.mean(scores)\n",
        "  print(\"평균정확도: {0:.4f}\".format(mean_score))\n",
        "# exec_kfold 호출\n",
        "exec_kfold(dt_clf, folds=5)"
      ],
      "metadata": {
        "id": "kNVrbBARLhOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "scores = cross_val_score (dt_clf, X_titanic_df, y_titanic_df, cv=5)\n",
        "for iter_count, accuracy in enumerate(scores):\n",
        "  print('교차검증{0}정확도:{1:4f}'.format(iter_count,accuracy))\n",
        "\n",
        "print(\"평균정확도:{0:4f}\".format(np.mean(scores)))"
      ],
      "metadata": {
        "id": "3qZw2Xm1MRLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "parameters = {'max_depth': [2, 3, 5, 10],\n",
        "              'min_samples_split': [2, 3, 5], 'min_samples_leaf': [1, 5, 8]}\n",
        "\n",
        "grid_dclf = GridSearchCV(dt_clf, param_grid=parameters, scoring='accuracy', cv=5)\n",
        "grid_dclf.fit(X_train, y_train)\n",
        "\n",
        "print('Gridsearchcv 최적 하이퍼 파라미터:', grid_dclf.best_params_)\n",
        "print('GridsearchcV 최고 정확도: {0:.4f}'.format(grid_dclf.best_score_))\n",
        "best_dclf = grid_dclf.best_estimator_\n",
        "\n",
        "# Gridsearchcv의 최적 하이퍼 파라미터로 학습된 Eestimator로 예측 및 평가 수행 .\n",
        "dpredictions = best_dclf.predict(X_test)\n",
        "accuracy =accuracy_score(y_test, dpredictions)\n",
        "print('테스트 세트에서의 DecisionTreeclassifier 정확도: {0:4f}'.format(accuracy))"
      ],
      "metadata": {
        "id": "qJ9J2a7vMaX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##07. 정리\n",
        "머신러닝 애플리케이션은\n",
        "1. 데이터의 가공 및 변환 과정의 전처리 작업\n",
        "2. 데이터를 학습 데이터와 테스트 데이터로 분리하는 데이터 세트 분리 작업\n",
        "3. 학습 데이터를 기반으로 머신러닝 알고리즘을 적용해 모델 학습\n",
        "4. 학습된 모델을 기반으로 테스트 데이터에 대한 예측 수행\n",
        "5. 예측된 결과값을 실제 결과값과 비교해 머신러닝 모델에 대한 평가 수행\n",
        "의 과정으로 구성\n",
        "\n",
        "#### 데이터의 전처리 작업\n",
        ": 오류 데이터의 보정이나 결손값(Null) 처리 등의 다앙한 데이터 클렌징 작업. 레이블 인코딩이나 원-핫인코딩과 같은 인코딩 작업, 그리고 데이터의 스케일링/정규화 작업 등으로 머신러닝 알고리즘이 최적으로 수행 될 수 있게 데이터를 사전 처리하는 것\n",
        "\n",
        "\n",
        "\n",
        "####ㅇ\n",
        "머신러닝 모델은 학습 데이터 세트로 학습한 뒤 별도의 테스트 데이터 세트로 평가되어야 함.\n",
        "\n",
        "또한, 테스트 데이터의 건수 부족이나 고정된 테스트 데이터 세트를 이용한 반복적인 모델의 학습과 평가는 해당 테스트 데이터 세트에만 치우친 빈약한 머신러닝 모델을 만들 가능성이 높습니다.\n",
        "\n",
        "이를 해결하기 위해 학습 데이터 세트를 학습 데이터와 검증 데이터로 구성된 여러 개의 폴드 세트로 분리해 교차검증을 수행할 수 있습니다.\n",
        "\n",
        "\n",
        "#### 교차검증\n",
        ": KFold, StratifiedkFold, cross_val_score()등의 다양한 클래스와 함수를 제공.\n",
        "또한 머신러닝모델의 최적의 하이퍼 파라미터를 교차검증을 통해 추출하기 위해 GridSearchcV를 제공."
      ],
      "metadata": {
        "id": "rvYAp8e1g6vm"
      }
    }
  ]
}